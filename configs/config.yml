experiment_name: "simple run"

models_to_run:
  - MLP
  - FAN
  - FANGated
  - FANPhaseOffsetModel
  - FANPhaseOffsetModelGated
#  - FANAmplitudePhaseModel
#  - FANUnconstrainedBasisModel

datasets_to_run:
  - sin
  - mod
  - complex_1
  - complex_2
  - complex_3
#  - complex_4
#  - combined_freq_amp_modulation
#  - gradually_increasing_amplitude
#  - gradually_increasing_frequency


data_versions:
  - original
  - noisy
#  - sparse

hyperparameters:
  # Hardware-adaptive hyperparameters (works for both CUDA and MPS)
  early_stopping: True
  early_stopping_patience: 300
  early_stopping_min_delta: 0.00001
  num_samples: 20000
  test_samples: 4000
  noise_level: 0.15
  sparsity_factor: 0.5
  epochs: 10000             # Increased epochs for longer training
  lr: 0.0001             # Lower initial learning rate for more stable training
  min_lr: 1e-7            # Minimum learning rate to prevent tiny gradient issues
  weight_decay: 0.00001   # L2 regularization for better generalization (as float)
  batch_size: 128         # Larger batch size for better hardware utilization
  num_workers: 4          # Use multiple CPU workers for data loading
  use_amp: true           # Automatic mixed precision (better for Tesla V100)
  optimizer: "adamw"      # Use AdamW optimizer for better performance
  use_scheduler: true     # Enable learning rate scheduling
  scheduler_type: "cosine" # Cosine annealing scheduler
  drop_last: true         # Drop last batch for more consistent batch sizes
  clip_value: 0.5         # Value for gradient clipping to prevent NaN issues
  clip_gradients: true    # Always clip gradients to prevent explosions
  multigpu: true          # Enable multi-GPU training
  gradient_accumulation_steps: 4   # Increased gradient accumulation for stability
  distributed_training: auto       # 'auto' (detect), true, false (use DataParallel)
  
  # NaN loss prevention settings
  nan_detection: true              # Enable advanced NaN detection and handling
  scheduler_patience: 10           # Increased patience for more stable LR reduction
  scheduler_factor: 0.5            # Less aggressive LR reduction factor
  
  # Model saving options
  resume_training: False            # Enable to automatically resume from latest checkpoint
  save_model: False                # Whether to save models at all (off by default to save disk space)
  
  # AWS p3dn.24xlarge specific configuration
  aws_p3dn_optimization: true     # Automatically optimize for AWS p3dn.24xlarge with 8x Tesla V100
  aws_max_gpus: 8                 # Maximum number of GPUs to use on AWS
  
  # Windows with Gigabyte GPU specific configuration
  windows_gigabyte_optimization: true  # Automatically optimize for Windows with Gigabyte GPUs
  windows_batch_size: 64              # Batch size optimized for Gigabyte GPUs
  windows_memory_optimization: true   # Apply memory optimizations for Gigabyte GPUs

logging:
  level: "INFO"  # Could be DEBUG, INFO, WARNING, ERROR, etc.

random_seed: 42

# Performance settings
# 'fast': Maximizes speed on hardware like Tesla V100 GPUs
# 'deterministic': Ensures reproducible results, but slower
performance_setting: 'fast'

# CUDA forced usage settings for EC2 p3.8x instances
force_cuda: true  # Try to use CUDA even when detection fails
bypass_pytorch_cuda_check: true  # Uses monkey patching to override PyTorch's CUDA detection
cuda_version: "12.4"  # Specify CUDA version installed on the instance
experiment_name: "reproduce2"

models_to_run:
#  - FANPhaseOffsetModelGated
#  - FANAmplitudePhaseModel
#  - FANMultiFrequencyModel
#  - FANUnconstrainedBasisModel
  - FANPhaseOffsetModel
#  - FANGated
#  - FAN
#  - MLP
#  - FANPhaseOffsetModelGated
#  - FANPhaseOffsetModel
#  - FANGated
#  - FAN
#  - MLP

datasets_to_run:
#  - complex_3
#  - sin
#  - increasing_amp_freq
#  - mod
#  - complex_2
#  - complex_1
#  - complex_4
  - combined_compression_amp
  - increasing_decreasing_amp
  - compressing_expanding_wave


data_versions:
  - original
#  - noisy
#  - sparse

hyperparameters:
  # Hardware-adaptive hyperparameters (works for both CUDA and MPS)
  early_stopping: false
  early_stopping_patience: 300
  early_stopping_min_delta: 0.000001
  num_samples: 10000
  test_samples: 4000
  noise_level: 0.15
  sparsity_factor: 0.5
  epochs: 100
  lr: 0.001              # Higher learning rate for faster convergence
  weight_decay: 0.00001   # L2 regularization for better generalization (as float)
  batch_size: 256         # Larger batch size for better hardware utilization
  num_workers: 4          # Use multiple CPU workers for data loading
  use_amp: false          # Automatic mixed precision (will auto-enable for CUDA)
  optimizer: "adamw"      # Use AdamW optimizer for better performance
  use_scheduler: true     # Enable learning rate scheduling
  scheduler_type: "cosine" # Cosine annealing scheduler
  drop_last: true         # Drop last batch for more consistent batch sizes

logging:
  level: "INFO"  # Could be DEBUG, INFO, WARNING, ERROR, etc.

random_seed: 42

# Performance settings
# 'fast': Maximizes speed on hardware like Tesla V100 GPUs
# 'deterministic': Ensures reproducible results, but slower
performance_setting: 'fast'

# CUDA forced usage settings for EC2 p3.8x instances
force_cuda: true  # Try to use CUDA even when detection fails
bypass_pytorch_cuda_check: true  # Uses monkey patching to override PyTorch's CUDA detection
cuda_version: "12.4"  # Specify CUDA version installed on the instance
experiment_name: "gated fix for below"

models_to_run:
#  - MLP
#  - FAN
#  - FANGated
#  - FANPhaseOffsetModel
#  - FANPhaseOffsetModelGated
  - FANPhaseOffsetModelUniform
#  - FANAmplitudePhaseModel
#  - FANUnconstrainedBasisModel

datasets_to_run:
  - sin
  - mod
  - complex_1
  - complex_2
  - complex_3
#  - complex_4
#  - combined_freq_amp_modulation
#  - gradually_increasing_amplitude
#  - gradually_increasing_frequency


data_versions:
  - original
  - noisy
#  - sparse

hyperparameters:
  # Hardware-adaptive hyperparameters optimized for NVIDIA GeForce GTX 1660
  early_stopping: True
  early_stopping_patience: 300
  early_stopping_min_delta: 0.00001
  num_samples: 20000
  test_samples: 4000
  noise_level: 0.15
  sparsity_factor: 0.5
  epochs: 10000             # Increased epochs for longer training
  lr: 0.0001             # Lower initial learning rate for more stable training
  min_lr: 1e-7            # Minimum learning rate to prevent tiny gradient issues
  weight_decay: 0.00001   # L2 regularization for better generalization (as float)
  batch_size: 64          # Reduced batch size for GTX 1660 (6GB VRAM)
  num_workers: 2          # Reduced workers to avoid overloading the system
  use_amp: true           # Automatic mixed precision for better performance
  optimizer: "adamw"      # Use AdamW optimizer for better performance
  use_scheduler: true     # Enable learning rate scheduling
  scheduler_type: "cosine" # Cosine annealing scheduler
  drop_last: true         # Drop last batch for more consistent batch sizes
  clip_value: 0.5         # Value for gradient clipping to prevent NaN issues
  clip_gradients: true    # Always clip gradients to prevent explosions
  multigpu: false         # Disable multi-GPU for single GPU setup
  gradient_accumulation_steps: 2   # Reduced for single GPU
  distributed_training: false      # Disable distributed training for single GPU
  device: "auto"          # Auto-detect the best available device
  
  # NaN loss prevention settings
  nan_detection: true              # Enable advanced NaN detection and handling
  scheduler_patience: 10           # Increased patience for more stable LR reduction
  scheduler_factor: 0.5            # Less aggressive LR reduction factor
  
  # Model saving options
  resume_training: False            # Enable to automatically resume from latest checkpoint
  save_model: False                # Whether to save models at all (off by default to save disk space)
  
  # AWS p3dn.24xlarge specific configuration - disabled for GTX 1660
  aws_p3dn_optimization: false     # Disable AWS optimizations
  aws_max_gpus: 1                  # Single GPU
  
  # Windows with GPU specific configuration
  windows_gigabyte_optimization: true  # Enable GPU optimizations
  windows_batch_size: 64              # Batch size optimized for GTX 1660
  windows_memory_optimization: true   # Apply memory optimizations

logging:
  level: "INFO"  # Could be DEBUG, INFO, WARNING, ERROR, etc.

random_seed: 42
# Performance settings
# 'fast': Maximizes speed on hardware like Tesla V100 GPUs
# 'deterministic': Ensures reproducible results, but slower
performance_setting: 'fast'

# Device detection settings
force_cuda: false  # Set to false to let PyTorch auto-detect available devices
bypass_pytorch_cuda_check: false  # Disable monkey patching of PyTorch's CUDA detection
cuda_version: "12.4"  # Specify CUDA version installed on the instance if available
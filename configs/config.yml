experiment_name: "new graphs"

models_to_run:
#  - MLP
  - FAN
  - FANGated
  - FANPhaseOffsetModel
  - FANPhaseOffsetModelGated
  - FANAmplitudePhaseModel
  - FANUnconstrainedBasisModel

datasets_to_run:
  - sin
  - mod
  - complex_1
  - complex_2
  - complex_3
  - complex_4
  - combined_freq_amp_modulation
  - gradually_increasing_amplitude
  - gradually_increasing_frequency


data_versions:
  - original
  - noisy
#  - sparse

hyperparameters:
  # Hardware-adaptive hyperparameters (works for both CUDA and MPS)
  early_stopping: False
  early_stopping_patience: 300
  early_stopping_min_delta: 0.000001
  num_samples: 50000
  test_samples: 10000
  noise_level: 0.15
  sparsity_factor: 0.5
  epochs: 10
  lr: 0.0001              # Original learning rate for FAN
  weight_decay: 0.00001   # L2 regularization for better generalization (as float)
  batch_size: 256         # Larger batch size for better hardware utilization
  num_workers: 4          # Use multiple CPU workers for data loading
  use_amp: false          # Automatic mixed precision (will auto-enable for CUDA)
  optimizer: "adamw"      # Use AdamW optimizer for better performance
  use_scheduler: true     # Enable learning rate scheduling
  scheduler_type: "cosine" # Cosine annealing scheduler
  drop_last: true         # Drop last batch for more consistent batch sizes
  clip_value: 0.5         # Value for gradient clipping to prevent NaN issues
  multigpu: true                # Enable multi-GPU training
  gradient_accumulation_steps: 2   # Number of batches to accumulate gradients for stability
  distributed_training: auto       # 'auto' (detect), true, false (use DataParallel)
  
  # AWS p3dn.24xlarge specific configuration
  aws_p3dn_optimization: true     # Automatically optimize for AWS p3dn.24xlarge with 8x Tesla V100
  aws_max_gpus: 8                 # Maximum number of GPUs to use on AWS

logging:
  level: "INFO"  # Could be DEBUG, INFO, WARNING, ERROR, etc.

random_seed: 42

# Performance settings
# 'fast': Maximizes speed on hardware like Tesla V100 GPUs
# 'deterministic': Ensures reproducible results, but slower
performance_setting: 'fast'

# CUDA forced usage settings for EC2 p3.8x instances
force_cuda: true  # Try to use CUDA even when detection fails
bypass_pytorch_cuda_check: true  # Uses monkey patching to override PyTorch's CUDA detection
cuda_version: "12.4"  # Specify CUDA version installed on the instance